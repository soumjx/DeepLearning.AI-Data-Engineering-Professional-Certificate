


---------


# Program Syllabus

- https://www.coursera.org/learn/intro-to-data-engineering/supplement/GScau/program-syllabus
- https://www.w3schools.com/python/pandas/default.asp
- https://www.kaggle.com/learn/pandas
- https://sqlbolt.com/

----------

# Data Engineering Defined — Key Points

## What Data Engineering Is
- Modern data engineering evolved from software engineers who shifted focus from applications to data systems.
- Organizations recognized data as valuable, leading to dedicated roles for handling large volumes and diverse sources.

## Definition (Fundamentals of Data Engineering)
**Data engineering = Developing, implementing, and maintaining systems/processes that convert raw data into high-quality, consistent information for downstream use (analytics, ML).**

## Core Areas Intersecting Data Engineering
- Security  
- Data Management  
- DataOps  
- Data Architecture  
- Orchestration  
- Software Engineering

## Data Engineering Life Cycle — Stages
- **Data Generation & Source Systems**  
- **Ingestion**  
- **Storage**  
- **Transformation**  
- **Serving**

> Storage underpins ingestion, transformation, and serving.

## End Use Cases
- Analytics  
- Machine Learning  
- Reverse ETL (sending processed data back to source systems)

## Data Pipeline (Concept)
- Combined architecture, systems, and processes that move data from generation → ingestion → storage → transformation → serving → use cases.

## Undercurrents (Apply Across All Stages)
- Security  
- Data Management  
- DataOps  
- Data Architecture  
- Orchestration  
- Software Engineering

## Core Responsibility of a Data Engineer
- Acquire raw data  
- Transform it into useful information  
- Serve it to downstream stakeholders



-------

# Data Engineering Defined — Essential Notes

## Evolution of Data Engineering
- Original data engineers were software engineers building software applications.
- Data generated by applications was regarded as a byproduct or exhaust.
- Over time, organizations recognized the intrinsic value of data.
- As volume and variety of data grew, engineers focused on building systems for ingesting, storing, transforming, and serving data.
- The role of data engineer emerged.

## Definition of Data Engineering
**Data engineering is the development, implementation, and maintenance of systems and processes that take in raw data and produce high quality, consistent information that supports downstream use cases, such as analysis and machine learning.**

- Data engineering is the intersection of:
  - Security  
  - Data management  
  - DataOps  
  - Data architecture  
  - Orchestration  
  - Software engineering

## Data Engineering Life Cycle
- The life cycle comprises stages.
- Data generation and source systems start the process.
- Middle stages: ingestion, transformation, storage, serving.
- Storage sits underneath ingestion, transformation, and serving, and spans the width of the box.
- Life cycle helps visualize common elements of any data system.
- End use cases: analytics, machine learning, Reverse ETL.

## Data Pipeline
- Data pipeline describes architecture, systems, and processes moving data through the stages of the data engineering life cycle.
- As a data engineer, you manage the life cycle from source systems to serving data for use cases.

## Undercurrents of the Life Cycle
- Six components presented as undercurrents:
  - Security  
  - Data management  
  - DataOps  
  - Data architecture  
  - Orchestration  
  - Software engineering
- Undercurrents span the entire life cycle.

## Holistic Thinking
- Avoid jumping straight into implementation and tools.
- Focus on higher level goals and how to provide value.
- Think holistically about the data engineering life cycle and undercurrents to transform stakeholder needs into system requirements.



--------

Video 


# A Brief History of Data Engineering — Video

## Data Everywhere
- Data comprises the building blocks of information.
- Data can be digitally recorded and stored or transmitted.

## Beginning of Digital Data
- Digital data begins in the 1960s with the advent of computers.
- First computerized databases introduced.

## 1970s
- Relational databases emerged.
- SQL developed at IBM.

## 1980s
- Bill Inmon developed the first data warehouse for analytical decision making.

## 1990s
- Growth of data systems created the need for reporting and business intelligence.
- Ralph Kimball and Bill Inmon developed data modeling approaches for analytics.
- Internet went mainstream → rise of web-first companies.
- Emergence of backend systems: servers, databases, storage solutions.

## Early 2000s
- After the dotcom bust, companies like Yahoo, Google, and Amazon faced exploding data needs.
- Traditional relational databases and warehouses could not handle this scale.
- Beginning of the big data era.

## Big Data
- Defined by very large datasets analyzed computationally.
- Described by the three Vs: velocity, variety, volume.

## 2004 – MapReduce
- Google published MapReduce, an ultra-scalable data processing paradigm.
- Marked a major milestone in data technologies and the roots of modern data engineering.

## 2006 – Hadoop
- Yahoo engineers developed and open-sourced Apache Hadoop.
- Enabled large-scale data processing and storage.
- Era of the big data engineer began.

## Amazon’s Contributions
- Amazon developed EC2, S3, DynamoDB, and core scalable data building blocks.
- Offered these as AWS, the first popular public cloud.
- Developers could rent compute and storage instead of managing hardware.
- Other public clouds followed: Google Cloud Platform and Microsoft Azure.
- Public cloud became a major innovation for data systems.

## Impact of Big Data Tools and Public Cloud
- Formed the foundation of today’s data ecosystem.
- Enabled small startups to access advanced data tools.

## Transition to Real-Time Data
- Shift from batch computing to event streaming.
- Enabled continuous processing of individual events.
- Era of big, real-time data.

## Decline of “Big Data” as a Term
- Managing early big data tools required heavy maintenance and large engineering teams.
- Companies spent excessive effort maintaining systems instead of delivering insights.
- Big data processing became accessible enough that the term became unnecessary.
- Big data engineers became simply data engineers.

## 2010s Onward
- Emergence of cloud-first, open source, and third-party products simplified working with data at scale.
- Data sources and formats continued to grow in variety and size.
- Data engineering became a discipline of interoperation and connecting technologies.

## Role of the Data Engineer Today
- Positioned further up the value chain.
- Ability to build powerful, scalable data systems using modern tools.
- Opportunity to contribute to future tools and technologies.
- Building robust data systems is central to business strategy.
- Data engineers directly support achieving business goals and delivering value.

## What Comes Next
- How data engineering fits with other roles and stakeholders.
- How to identify end users and their needs.
- How this relates to business value.
- How to translate stakeholder needs into system requirements.


----------


# The Data Engineer Among Other Stakeholders — Essential Notes

## Downstream Stakeholders
- Data engineers must understand downstream needs to turn raw data into something useful.
- Downstream data consumers may include:
  - Analysts
  - Data scientists
  - Machine learning engineers
  - Sales, product, marketing, executives

### Example Requirements from a Business Analyst
- Need to run SQL queries for analysis, dashboards, trends, predictions.
- Requirements to clarify:
  - Query frequency for dashboard refresh
  - Information needed per query
  - Useful joins or aggregations to run ahead of time
  - Acceptable latency (hour-old, day-old, real time)
  - Agreed definitions of metrics (e.g., time zones and start/end times for daily sales)

### Importance
- Understanding company strategy helps identify business value in the data served.
- Data engineers must understand business metrics and stakeholder goals.

## Upstream Stakeholders
- Upstream stakeholders develop and maintain the source systems that generate raw data.
- Typically software engineers (internal or third-party).
- Data engineers act as data consumers; source system owners serve them.

### Upstream Requirements to Clarify
- Volume, frequency, and format of generated data  
- Data security and regulatory compliance  
- Advance notice of disruptions, outages, schema changes

### Collaboration Benefits
- Open communication can influence how raw data is served.
- Helps mitigate disruptions to data pipelines.

## Recap
- Data engineers must understand both upstream and downstream stakeholders.
- Connect with source system owners to understand incoming data and potential disruptions.
- Understand downstream goals, business value, and how data supports the organization.

## Next Step
- Next video discusses business value.
- Optional video; can skip ahead to requirements gathering.

<img width="1852" height="829" alt="image" src="https://github.com/user-attachments/assets/eb18e3b5-dad8-4a72-a6bc-80c134cec4c7" />



-----------



# Requirements Gathering Conversation — Essential Notes

## Purpose
- Demonstrates an example of a requirements gathering conversation between a data engineer and a key stakeholder (data scientist).

## Stakeholder Context
- Data scientist needs support for:
  - Real-time analysis of product sales by region.
  - Improved data ingestion and processing.
  - Better support for dashboards and recommendation systems.

## Current Challenges Described by Stakeholder
- Sales data stored in production database; software engineers restrict direct access.
- Stakeholder receives **daily data dumps** (CSV, JSON), must manually pull, clean, extract, and aggregate data.
- ~90% of data received is unnecessary; ~80% of time spent processing data.
- Scripts crash due to anomalies or schema changes from software team.
- Data available is typically **two days old**; marketing team wants **real-time** or near real-time.
- Manual processing causes delays and unpredictability.

## Use Cases Identified
### 1. **Dashboards for Marketing**
- Show product sales by category and region over the past 30 days.
- Include totals and timeline plots (daily/hourly).
- Marketing wants **current metrics**, not delayed data.

### 2. **Recommendation Engine**
- Currently serving popular product recommendations (not personalized).
- Plans to train a personalized recommender model.
- Needs:
  - More training data
  - A system to serve user activity data to the model
  - Serving of recommendation outputs back to the platform in real time

## Actions Needed (Implied Requirements)
- More direct and timely ingestion of data from the sales database.
- Automated, orchestrated transformation and processing to avoid manual work.
- Serve cleaned, structured data in required formats for:
  - Dashboards
  - Recommendation engine
- Handle schema changes and anomalies consistently.
- Reduce data latency according to marketing needs (exact tolerance requires follow-up).

## Follow-Up Needed
- Clarify from marketing what “real-time” means (seconds/minutes/hour/today-level).
- Understand what actions marketing will take with more current data.
- Translate stakeholder goals into:
  - Functional requirements (what the system must do)
  - Non-functional requirements (latency, robustness, automation)

## Summary of Agreement
- Automated ingestion + automated processing + serving in correct format woul



------------



# Data Engineering on the Cloud — Essential Notes

## High-Level Foundation
- Week 1 focused on high-level concepts:
  - Data engineering life cycle and undercurrents  
  - History of data engineering  
  - Stakeholder collaboration and delivering value  
- This high-level mindset is foundational and revisited throughout the specialization.

## Tools and Technologies
- Data engineering tools vary widely between companies.
- With the advent of the public cloud and many open-source/proprietary tools, companies of all sizes now use similar high-performance technologies.

## Cloud vs On-Premises
- Historically, companies built and maintained **on-premises** data infrastructure.
- Many companies today build their systems **entirely on the Cloud**.
- Some companies still maintain on-premises systems due to:
  - Regulatory constraints  
  - Legacy systems they prefer to maintain  
- Data engineers may encounter on-premises systems or be tasked with **migrating** them to the Cloud.

## Cloud First Reality
- Increasing likelihood that data engineering work happens **exclusively on the Cloud**.
- AWS is the first and most widely used public cloud.
- Other providers include Google Cloud Platform and Microsoft Azure.

## Approach in This Specialization
- Cloud-first approach; no deep focus on on-premises or migration considerations.
- Partnered with **AWS** for hands-on labs.
- You will build data pipelines and architectures using AWS tools used widely in industry.

## Learning Goals
- By the end of Course 1:
  - Take a set of technical requirements.
  - Stand up a **Cloud-based data pipeline** using appropriate AWS tools.

## AWS Learning Approach
- AWS has a large set of tools and resources.
- The specialization uses a **just-in-time** approach:
  - Learn specific tools only when needed in labs.
  - No prior Cloud experience required.

## Upcoming
- Introduction to AWS tools and resources with **Morgan Willis**, principal Cloud technologist.

--------

# Intro to the AWS Cloud — Essential Notes

## What the AWS Cloud Is
- AWS describes the cloud as **on-demand delivery of IT resources over the Internet with pay-as-you-go pricing**.
- Resources can be provisioned instantly and shut down when no longer needed.
- You only pay for what you use.

## Difference from On-Premises
- On-premises systems require long-term commitment to hardware.
- AWS avoids upfront purchasing and long-term maintenance of data centers.

## Core IT Resources on AWS
### Compute (run code)
- Virtual machines
- Container hosting services
- Serverless functions

### Storage (store data)
- Amazon S3 (object storage)
- Amazon Elastic Block Store (block storage)
- Databases:
  - Relational
  - NoSQL
  - Graph, etc.

### Networking (connect resources)
- Amazon Virtual Private Cloud (VPC) — private network in the cloud.

## Additional AWS Categories
- Security
- Data streaming
- Ingestion
- Transformation
- Other specialized services

## Scalability and Elasticity
- Cloud resources scale **up and down automatically**.
- No need to predict capacity in advance.
- Handles spikes in demand.
- Reduces cost by avoiding unused resources.

## Analogy
- AWS works like electricity consumption:
  - Pay for usage
  - Implementation details are abstracted away

## AWS Global Infrastructure
- AWS services run in physical data centers globally.
- You interact with them via **APIs**.

### Regions
- Geographical areas containing multiple data centers.
- Examples:  
  - US East (N. Virginia)  
  - Asia Pacific (Mumbai)  
  - Europe (Frankfurt)

### Availability Zones (AZs)
- Each region has multiple AZs.
- Each AZ contains multiple data centers.
- AZs are isolated enough to withstand localized failures.
- Designed for high availability and failover.

### Connectivity
- Regions and AZs are connected by low-latency AWS global network fiber links.

## Building on AWS
- Data engineers combine multiple AWS services like building blocks.
- AWS has **200+ services**:
  - General purpose
  - Specialized

## Summary
- Concepts covered (regions, AZs, scaling, core resources) recur throughout AWS learning.
- Next: core AWS services used in these courses.


--------

 # AWS Regions and Availability Zones
 https://www.coursera.org/learn/intro-to-data-engineering/supplement/1WSy7/aws-regions-and-availability-zones


--------
# Intro to AWS Core Services — Essential Notes

## Core Service Categories (overview order)
- **Compute**
- **Network**
- **Storage**
- **Databases**
- **Security**

## Compute
- **Amazon Elastic Compute Cloud (EC2):** provides virtual machines (VMs) → EC2 instance = launched VM.
  - Complete control over OS, applications, instance.
  - Use cases: development machine, web server, containers, ML workloads.
  - Scale horizontally by deploying single instance or fleet.
- Other compute options:
  - **AWS Lambda** (serverless functions — code runs in response to triggers/events)
  - **Amazon ECS / EKS** (container hosting services)

## Network
- **Amazon Virtual Private Cloud (VPC):** private network in the cloud.
  - Choose private IP space and partition into **subnets**.
  - A single VPC spans all AZs within a region; cannot span regions.
- Most AWS resources are **region-bound**.
  - Data and resources do not leave a region unless explicitly designed to do so.
- When creating resources (e.g., EC2, instance-based databases), select **VPC** and **AZ**.

## Storage
- Storage types: **Object**, **Block**, **File**.
- **Object storage:** used for unstructured data (documents, logs, photos, videos); Amazon S3 is the object store referenced.
- **Block storage:** for databases, VM file systems, low-latency/high-performance needs.
  - **Amazon EBS** volumes attach to EC2 instances and mount to the OS.
- **File storage:** hierarchical files/directories; **Amazon EFS** provides scalable file storage mountable by multiple systems.

## Databases
- Databases provide structured-data management, complex querying, indexing (distinct from general storage).
- Key services in these courses:
  - **Amazon RDS** (Relational Database Service)
  - **Amazon Redshift** (data warehouse for storing, transforming, and serving data)
- Other AWS database services exist; RDS and Redshift are primary here.

## Security — Shared Responsibility Model
- **Shared responsibility model:** AWS = security *of* the cloud; you = security *in* the cloud.
- Analogy: apartment building (owner secures building; tenant uses locks).
- Example (EC2):
  - AWS responsible for physical hardware, facilities, hypervisor layer.
  - You responsible for guest OS management, software updates/patching, networking config (firewalls), data access, encryption.
- Each AWS service draws the responsibility line differently.

## Labs and Optional Account
- Labs provide access to AWS resources; creating a personal AWS account is optional.
- Next: walkthrough of the AWS Management Console (optional account setup instructions available).


--------
# Compute - Amazon Elastic Compute Cloud (EC2)
https://www.coursera.org/learn/intro-to-data-engineering/supplement/fyNJD/compute-amazon-elastic-compute-cloud-ec2

--------


--------



--------


--------



--------


--------
